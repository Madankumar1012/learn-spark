{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RFM Segmentation\n",
    "https://clevertap.com/blog/rfm-analysis/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Acquisition & Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modeling - Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "startTime = 50593955851584\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "50593955851584"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val startTime = System.nanoTime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schema = StructType(StructField(customer_id,StringType,false), StructField(purchase_amount,DoubleType,false), StructField(date_of_purchase,DateType,false))\n",
       "data = [customer_id: string, purchase_amount: double ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[customer_id: string, purchase_amount: double ... 1 more field]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val schema = StructType(\n",
    "                List(\n",
    "                    StructField(\"customer_id\", StringType, false),\n",
    "                    StructField(\"purchase_amount\", DoubleType, false),\n",
    "                    StructField(\"date_of_purchase\", DateType, false)\n",
    "                )\n",
    "            )\n",
    "val data = spark.read\n",
    "                .option(\"sep\", \"\\t\")\n",
    "                .option(\"mode\",\"FAILFAST\")\n",
    "                .option(\"dateFormat\",\"YYYY-MM-dd\")\n",
    "                .schema(schema)\n",
    "                .csv(\"../../data/foundation-marketing-analytics/purchases.txt\")\n",
    "                .toDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derive days_since col for RECENCY calculation later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+----------------+----------+----------+\n",
      "|customer_id|purchase_amount|date_of_purchase|  end_date|days_since|\n",
      "+-----------+---------------+----------------+----------+----------+\n",
      "|        760|           25.0|      2009-11-06|2016-01-01|      2247|\n",
      "|        860|           50.0|      2012-09-28|2016-01-01|      1190|\n",
      "|       1200|          100.0|      2005-10-25|2016-01-01|      3720|\n",
      "|       1420|           50.0|      2009-07-09|2016-01-01|      2367|\n",
      "|       1940|           70.0|      2013-01-25|2016-01-01|      1071|\n",
      "+-----------+---------------+----------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "enriched1 = [customer_id: string, purchase_amount: double ... 3 more fields]\n",
       "nullCount = 0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "//Why is end_date set to \"2016-01-01\"?\n",
    "\n",
    "val enriched1 = data\n",
    "                .withColumn(\"end_date\", lit(\"2016-01-01\"))\n",
    "                .withColumn(\"days_since\", datediff($\"end_date\", $\"date_of_purchase\"))\n",
    "enriched1.show(5)\n",
    "\n",
    "//Verify if any calculations have failed\n",
    "val nullCount = enriched1.filter(isnull($\"days_since\")).count()\n",
    "assert(nullCount == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create features: FREQUENCY, RECENCY (in days) and Monetary value\n",
    "\n",
    "1. Recency: How recently a customer has made a purchase\n",
    "2. Frequency: How often a customer makes a purchase\n",
    "3. Monetary Value: How much money a customer spends on purchases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+---------+------+\n",
      "|customer_id|recency|frequency|amount|\n",
      "+-----------+-------+---------+------+\n",
      "|         90|    758|       10| 115.8|\n",
      "|         10|   3829|        1|  30.0|\n",
      "+-----------+-------+---------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "enriched2 = [customer_id: string, recency: int ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[customer_id: string, recency: int ... 2 more fields]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val enriched2 = enriched1\n",
    "                .groupBy($\"customer_id\")\n",
    "                .agg(\n",
    "                    min($\"days_since\").alias(\"recency\"),\n",
    "                    count($\"customer_id\").alias(\"frequency\"),\n",
    "                    avg($\"purchase_amount\").alias(\"amount\"))\n",
    "enriched2.filter($\"customer_id\".isin(\"10\", \"90\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let us do some summary/descriptive stats**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+\n",
      "|summary|       customer_id|           recency|         frequency|            amount|\n",
      "+-------+------------------+------------------+------------------+------------------+\n",
      "|  count|             18417|             18417|             18417|             18417|\n",
      "|   mean|137573.51088668077|  1253.03789976652|2.7823749796383774|57.792985101815624|\n",
      "| stddev|  69504.5998805089|1081.4378683668397| 2.936888270392829|154.36010930845674|\n",
      "|    min|                10|                 1|                 1|               5.0|\n",
      "|    max|             99990|              4014|                45|            4500.0|\n",
      "+-------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "enriched2.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+---------+------------------+\n",
      "|customer_id|recency|frequency|            amount|\n",
      "+-----------+-------+---------+------------------+\n",
      "|       6240|   3005|        3| 76.66666666666667|\n",
      "|      52800|   3320|        1|              15.0|\n",
      "|     100140|     13|        4|             51.25|\n",
      "|     109180|     30|        8|             48.75|\n",
      "|     131450|    205|        8|            103.75|\n",
      "|      45300|    234|        6|29.166666666666668|\n",
      "|      69460|     15|        9| 28.88888888888889|\n",
      "|      86180|      2|        9| 21.11111111111111|\n",
      "|     161110|   1528|        1|              30.0|\n",
      "|      60070|   2074|        3|51.666666666666664|\n",
      "|      13610|   1307|        8|           3043.75|\n",
      "|     100010|    413|        7|27.857142857142858|\n",
      "|     107930|    150|        5|              79.0|\n",
      "|     132610|     30|        7|28.571428571428573|\n",
      "|     154770|    427|        1|              45.0|\n",
      "|      49290|    371|        5|              24.0|\n",
      "|     229650|    419|        1|              45.0|\n",
      "|     220290|    366|        2|              40.0|\n",
      "|     178550|      3|        8|             56.25|\n",
      "|     112480|   1890|        3|38.333333333333336|\n",
      "+-----------+-------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "enriched2.createOrReplaceTempView(\"customers\")\n",
    "spark.sql(\"select * from customers\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "We need to address the following before we can train the model:\n",
    "\n",
    "1. Data Dispersion: The *amount* is skewed. We need to take **log** to address this skewness.\n",
    "2. Scale: Different features use different scales (see table). These features need to be standardized so that they contribute equally to the result.\n",
    "\n",
    "\n",
    "|Feature|Scale|\n",
    "|-------|-----|\n",
    "|recency|days|\n",
    "|frequency|purchase occassions|\n",
    "|amount|dollars|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Handle data dispersion**\n",
    "\n",
    "Below histogram shows that majority of the purchases are below 5 and 300 dollars. The data is left-skewed ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "startValues = Array(5.0, 304.6666666666667, 604.3333333333334, 904.0, 1203.6666666666667, 1503.3333333333333, 1803.0, 2102.6666666666665, 2402.3333333333335, 2702.0, 3001.6666666666665, 3301.3333333333335, 3601.0, 3900.6666666666665, 4200.333333333333, 4500.0)\n",
       "counts = Array(18126, 144, 28, 56, 16, 8, 19, 4, 3, 5, 1, 0, 0, 5, 2)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(18126, 144, 28, 56, 16, 8, 19, 4, 3, 5, 1, 0, 0, 5, 2)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val (startValues, counts) = enriched2.select($\"amount\").map(v=>v.getDouble(0)).rdd.histogram(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us derive a new column, *log_amount*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+---------+-----------------+------------------+\n",
      "|customer_id|recency|frequency|           amount|        log_amount|\n",
      "+-----------+-------+---------+-----------------+------------------+\n",
      "|       6240|   3005|        3|76.66666666666667| 4.339467020255086|\n",
      "|      52800|   3320|        1|             15.0|  2.70805020110221|\n",
      "|     100140|     13|        4|            51.25|3.9367156180185177|\n",
      "|     109180|     30|        8|            48.75| 3.886705197443856|\n",
      "|     131450|    205|        8|           103.75| 4.641984159110808|\n",
      "+-----------+-------+---------+-----------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "enriched3 = [customer_id: string, recency: int ... 3 more fields]\n",
       "startValues = Array(1.6094379124341003, 2.062930896655721, 2.516423880877342, 2.9699168650989627, 3.423409849320583, 3.8769028335422036, 4.330395817763825, 4.783888801985445, 5.237381786207066, 5.690874770428687, 6.1443677546503075, 6.597860738871929, 7.051353723093549, 7.50484670731517, 7.958339691536791, 8.411832675758411)\n",
       "counts = Array(109, 987, 1567, 6989, 3004, 3291, 1453, 487, 217, 95, 81, 73, 26, 26, 12)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(109, 987, 1567, 6989, 3004, 3291, 1453, 487, 217, 95, 81, 73, 26, 26, 12)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val enriched3 = enriched2.withColumn(\"log_amount\", log($\"amount\"))\n",
    "val (startValues, counts) = enriched3.select($\"log_amount\").map(v=>v.getDouble(0)).rdd.histogram(15)\n",
    "enriched3.createOrReplaceTempView(\"customers\")\n",
    "spark.sql(\"select * from customers\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Scale**\n",
    "\n",
    "Distance algorithms like K-means (clustering) are most affected by the range of features because they are using distance between data points to determine their similarity.  So there is a chance that higher weights will be given to features with higher magnitude.\n",
    "\n",
    "**Standardization** is a technique used to scale our features so that all the features **contribute equally to the result**. Logic: (data - mean)/std-dev. This puts most of the data between -2 & 2.\n",
    "\n",
    "We will use [StandardScaler](https://spark.apache.org/docs/1.4.1/ml-features.html#standardscaler) from Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+---------+-----------------+------------------+--------------------+\n",
      "|customer_id|recency|frequency|           amount|        log_amount|            features|\n",
      "+-----------+-------+---------+-----------------+------------------+--------------------+\n",
      "|       6240|   3005|        3|76.66666666666667| 4.339467020255086|[3005.0,3.0,4.339...|\n",
      "|      52800|   3320|        1|             15.0|  2.70805020110221|[3320.0,1.0,2.708...|\n",
      "|     100140|     13|        4|            51.25|3.9367156180185177|[13.0,4.0,3.93671...|\n",
      "+-----------+-------+---------+-----------------+------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----------+-------+---------+-----------------+------------------+------------------------------+-----------------------------------------------------------+\n",
      "|customer_id|recency|frequency|amount           |log_amount        |vectors                       |features                                                   |\n",
      "+-----------+-------+---------+-----------------+------------------+------------------------------+-----------------------------------------------------------+\n",
      "|6240       |3005   |3        |76.66666666666667|4.339467020255086 |[3005.0,3.0,4.339467020255086]|[1.6200302869727041,0.07410054463274257,0.9865583530258754]|\n",
      "|52800      |3320   |1        |15.0             |2.70805020110221  |[3320.0,1.0,2.70805020110221] |[1.911309156720168,-0.606892334858885,-1.1388099972602492] |\n",
      "|100140     |13     |4        |51.25            |3.9367156180185177|[13.0,4.0,3.9367156180185177] |[-1.146656628215909,0.4145969843785564,0.4618640668707043] |\n",
      "+-----------+-------+---------+-----------------+------------------+------------------------------+-----------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "assembler = vecAssembler_f240288b85fc\n",
       "vectors = [customer_id: string, recency: int ... 4 more fields]\n",
       "scaler = stdScal_475506728ef0\n",
       "scalerModel = stdScal_475506728ef0\n",
       "scaledFeatures = [customer_id: string, recency: int ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[customer_id: string, recency: int ... 5 more fields]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.StandardScaler\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "\n",
    "val assembler = new VectorAssembler()\n",
    "                    .setInputCols(Array(\"recency\", \"frequency\", \"log_amount\"))\n",
    "                    .setOutputCol(\"vectors\")\n",
    "\n",
    "val vectors = assembler.transform(enriched3)\n",
    "features.show(3)\n",
    "\n",
    "\n",
    "val scaler = new StandardScaler()\n",
    "                .setInputCol(\"vectors\")\n",
    "                .setOutputCol(\"features\")\n",
    "                .setWithStd(true)\n",
    "                .setWithMean(true)\n",
    "// Compute summary statistics by fitting the StandardScaler\n",
    "val scalerModel = scaler.fit(vectors)\n",
    "// Standardize each feature to have unit standard deviation.\n",
    "val scaledFeatures = scalerModel.transform(vectors)\n",
    "scaledFeatures.show(3, truncate=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modeling - Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the feature vector from *scaledFeatures* dataframe so that we can fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- recency: integer (nullable = true)\n",
      " |-- frequency: long (nullable = false)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- log_amount: double (nullable = true)\n",
      " |-- vectors: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.clustering.BisectingKMeans\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "\n",
    "//std_features col is of type vector\n",
    "scaledFeatures.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply [hierarchial clustering](http://spark.apache.org/docs/latest/mllib-clustering.html#bisecting-k-means) using BisectingKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bkm = bisecting-kmeans_6c8e1ab54200\n",
       "model = bisecting-kmeans_6c8e1ab54200\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "bisecting-kmeans_6c8e1ab54200"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Clustering the data into 9 clusters by BisectingKMeans.\n",
    "val bkm = new BisectingKMeans().setK(9)\n",
    "val model = bkm.fit(scaledFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within Set Sum of Squared Errors: 14319.292007940281\n",
      "Number of clusters: 9\n",
      "Number of iterations: 20\n",
      "Cluster centers:\n",
      "Cluster Center 0: [-0.8453714912512168,-0.1878697149175627,-0.4576033841040799]\n",
      "Cluster Center 1: [-0.6480140769348111,-0.20432490122092373,0.4307515929770496]\n",
      "Cluster Center 2: [-0.5669604365509189,-0.20728045846771836,1.8445461933131673]\n",
      "Cluster Center 3: [-0.8416225909972415,2.1718402417686407,-0.15558894779212123]\n",
      "Cluster Center 4: [-0.8453839898457399,1.8889976661701162,1.5212988435939618]\n",
      "Cluster Center 5: [0.045064047848971646,-0.4027017984943741,-0.6439217331189663]\n",
      "Cluster Center 6: [0.9001547309328005,-0.4303299639742082,-1.3296655550462693]\n",
      "Cluster Center 7: [1.0211281927777782,-0.4084553533731202,0.5060895525101492]\n",
      "Cluster Center 8: [1.617914928825506,-0.4970940188711775,-0.4932730234533967]\n",
      "Frequency (or number of data points) of each cluster:\n",
      "Cluser: 0, # of members: 3148\n",
      "Cluser: 1, # of members: 2666\n",
      "Cluser: 2, # of members: 1607\n",
      "Cluser: 3, # of members: 1741\n",
      "Cluser: 4, # of members: 733\n",
      "Cluser: 5, # of members: 2534\n",
      "Cluser: 6, # of members: 1460\n",
      "Cluser: 7, # of members: 2289\n",
      "Cluser: 8, # of members: 2239\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "summary = org.apache.spark.ml.clustering.BisectingKMeansSummary@65fa1276\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.ml.clustering.BisectingKMeansSummary@65fa1276"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Show the compute cost and the cluster centers\n",
    "println(s\"Within Set Sum of Squared Errors: ${model.computeCost(scaledFeatures)}\")\n",
    "\n",
    "val summary = model.summary\n",
    "\n",
    "println(s\"Number of clusters: ${summary.clusterSizes.length}\")\n",
    "println(s\"Number of iterations: ${summary.numIter}\")\n",
    "\n",
    "println(\"Cluster centers:\")\n",
    "model.clusterCenters.zipWithIndex.foreach { case (center, idx) =>\n",
    "  println(s\"Cluster Center ${idx}: ${center}\")\n",
    "}\n",
    "println(\"Frequency (or number of data points) of each cluster:\")\n",
    "summary.clusterSizes.zipWithIndex.foreach { case(size, idx) =>\n",
    "    println(s\"Cluser: ${idx}, # of members: ${size}\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modeling - Model Evaluation\n",
    "\n",
    "Accuracy is a useful metric in supervised learning, such as classification. However, in case of unsupervised learning (like KMeans, BisectingKMeans), there is no accuracy (as there is no labeled data / gold standard you can evaluate against.\n",
    "\n",
    "However, we can use the ClusteringEvaluator for assessing the quality of our model. You can find more information in the docs: https://spark.apache.org/docs/2.3.0/api/java/org/apache/spark/ml/evaluation/ClusteringEvaluator.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Segmentation summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- recency: integer (nullable = true)\n",
      " |-- frequency: long (nullable = false)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- log_amount: double (nullable = true)\n",
      " |-- vectors: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- prediction: integer (nullable = false)\n",
      "\n",
      "Show prediction for first 10 customers:\n",
      "+-----------+----------+\n",
      "|customer_id|prediction|\n",
      "+-----------+----------+\n",
      "|         10|         8|\n",
      "|         80|         4|\n",
      "|         90|         4|\n",
      "|        120|         5|\n",
      "|        130|         7|\n",
      "|        160|         8|\n",
      "|        190|         7|\n",
      "|        220|         5|\n",
      "|        230|         8|\n",
      "|        240|         0|\n",
      "+-----------+----------+\n",
      "\n",
      "Show profile of each segment:\n",
      "+-------+-----+-------+-----+-----+-----+-----+------------------+------+-----------------+\n",
      "|segment|min_r|avg_r  |max_r|min_f|avg_f|max_f|min_a             |avg_a |max_a            |\n",
      "+-------+-----+-------+-----+-----+-----+-----+------------------+------+-----------------+\n",
      "|0      |1    |338.82 |1563 |1    |2.23 |5    |5.0               |26.24 |38.0             |\n",
      "|1      |1    |552.25 |1892 |1    |2.18 |5    |31.666666666666668|50.75 |73.75            |\n",
      "|2      |1    |639.91 |3664 |1    |2.17 |5    |66.66666666666667 |245.61|4500.0           |\n",
      "|3      |1    |342.88 |2822 |6    |9.16 |45   |6.75              |34.5  |95.95238095238095|\n",
      "|4      |1    |338.81 |2557 |6    |8.33 |23   |54.166666666666664|156.4 |3043.75          |\n",
      "|5      |2    |1302.9 |2211 |1    |1.6  |7    |5.0               |23.45 |50.0             |\n",
      "|6      |413  |2222.01|3691 |1    |1.52 |9    |5.0               |13.74 |24.0             |\n",
      "|7      |1268 |2357.32|4012 |1    |1.58 |8    |27.5              |61.2  |1807.0           |\n",
      "|8      |2095 |3002.71|4014 |1    |1.32 |8    |8.0               |25.68 |55.0             |\n",
      "+-------+-----+-------+-----+-----+-----+-----+------------------+------+-----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "segments = [segment: int, min_r: int ... 8 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[segment: int, min_r: int ... 8 more fields]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary.predictions.printSchema()\n",
    "println(\"Prediction for first 10 customers:\")\n",
    "summary\n",
    "    .predictions\n",
    "    .withColumn(\"cust_id_numeric\", $\"customer_id\".cast(IntegerType))\n",
    "    .orderBy($\"cust_id_numeric\")\n",
    "    .select(\"customer_id\", \"prediction\")\n",
    "    .limit(10)\n",
    "    .show(10)\n",
    "\n",
    "println(\"Profile of each segment:\")\n",
    "val segments = summary\n",
    "                .predictions\n",
    "                .groupBy($\"prediction\".alias(\"segment\"))\n",
    "                .agg(\n",
    "                    min($\"recency\").alias(\"min_r\"),\n",
    "                    round(avg($\"recency\"),2).alias(\"avg_r\"),\n",
    "                    max($\"recency\").alias(\"max_r\"),\n",
    "                    \n",
    "                    min($\"frequency\").alias(\"min_f\"),\n",
    "                    round(avg($\"frequency\"),2).alias(\"avg_f\"),\n",
    "                    max($\"frequency\").alias(\"max_f\"),\n",
    "                    \n",
    "                    min($\"amount\").alias(\"min_a\"),\n",
    "                    round(avg($\"amount\"),2).alias(\"avg_a\"),\n",
    "                    max($\"amount\").alias(\"max_a\"))\n",
    "                .orderBy($\"segment\")\n",
    "segments.show(10, truncate=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 mins.\n",
      "177 secs.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "duration = 177045787900\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "177045787900"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val duration = System.nanoTime() - startTime\n",
    "import java.util.concurrent.TimeUnit\n",
    "\n",
    "println(TimeUnit.NANOSECONDS.toMinutes(duration) + \" mins.\")\n",
    "println(TimeUnit.NANOSECONDS.toSeconds(duration) + \" secs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
