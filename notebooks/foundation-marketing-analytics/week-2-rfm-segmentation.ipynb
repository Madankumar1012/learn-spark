{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RFM Segmentation\n",
    "https://clevertap.com/blog/rfm-analysis/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Acquisition & Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modeling - Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schema = StructType(StructField(customer_id,StringType,false), StructField(purchase_amount,DoubleType,false), StructField(date_of_purchase,DateType,false))\n",
       "data = [customer_id: string, purchase_amount: double ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[customer_id: string, purchase_amount: double ... 1 more field]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val schema = StructType(\n",
    "                List(\n",
    "                    StructField(\"customer_id\", StringType, false),\n",
    "                    StructField(\"purchase_amount\", DoubleType, false),\n",
    "                    StructField(\"date_of_purchase\", DateType, false)\n",
    "                )\n",
    "            )\n",
    "val data = spark.read\n",
    "                .option(\"sep\", \"\\t\")\n",
    "                .option(\"mode\",\"FAILFAST\")\n",
    "                .option(\"dateFormat\",\"YYYY-MM-dd\")\n",
    "                .schema(schema)\n",
    "                .csv(\"../../data/foundation-marketing-analytics/purchases.txt\")\n",
    "                .toDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derive days_since col for RECENCY calculation later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+----------------+----------+----------+\n",
      "|customer_id|purchase_amount|date_of_purchase|  end_date|days_since|\n",
      "+-----------+---------------+----------------+----------+----------+\n",
      "|        760|           25.0|      2009-11-06|2016-01-01|      2247|\n",
      "|        860|           50.0|      2012-09-28|2016-01-01|      1190|\n",
      "|       1200|          100.0|      2005-10-25|2016-01-01|      3720|\n",
      "|       1420|           50.0|      2009-07-09|2016-01-01|      2367|\n",
      "|       1940|           70.0|      2013-01-25|2016-01-01|      1071|\n",
      "+-----------+---------------+----------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "enriched1 = [customer_id: string, purchase_amount: double ... 3 more fields]\n",
       "nullCount = 0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "//Why is end_date set to \"2016-01-01\"?\n",
    "\n",
    "val enriched1 = data\n",
    "                .withColumn(\"end_date\", lit(\"2016-01-01\"))\n",
    "                .withColumn(\"days_since\", datediff($\"end_date\", $\"date_of_purchase\"))\n",
    "enriched1.show(5)\n",
    "\n",
    "//Verify if any calculations have failed\n",
    "val nullCount = enriched1.filter(isnull($\"days_since\")).count()\n",
    "assert(nullCount == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create features: FREQUENCY, RECENCY (in days) and Monetary value\n",
    "\n",
    "1. Recency: How recently a customer has made a purchase\n",
    "2. Frequency: How often a customer makes a purchase\n",
    "3. Monetary Value: How much money a customer spends on purchases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+---------+------+\n",
      "|customer_id|recency|frequency|amount|\n",
      "+-----------+-------+---------+------+\n",
      "|         90|    758|       10| 115.8|\n",
      "|         10|   3829|        1|  30.0|\n",
      "+-----------+-------+---------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "enriched2 = [customer_id: string, recency: int ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[customer_id: string, recency: int ... 2 more fields]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val enriched2 = enriched1\n",
    "                .groupBy($\"customer_id\")\n",
    "                .agg(\n",
    "                    min($\"days_since\").alias(\"recency\"),\n",
    "                    count($\"customer_id\").alias(\"frequency\"),\n",
    "                    avg($\"purchase_amount\").alias(\"amount\"))\n",
    "enriched2.filter($\"customer_id\".isin(\"10\", \"90\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let us do some summary/descriptive stats**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+\n",
      "|summary|       customer_id|           recency|         frequency|            amount|\n",
      "+-------+------------------+------------------+------------------+------------------+\n",
      "|  count|             18417|             18417|             18417|             18417|\n",
      "|   mean|137573.51088668077|  1253.03789976652|2.7823749796383774|57.792985101815624|\n",
      "| stddev|  69504.5998805089|1081.4378683668397| 2.936888270392829|154.36010930845674|\n",
      "|    min|                10|                 1|                 1|               5.0|\n",
      "|    max|             99990|              4014|                45|            4500.0|\n",
      "+-------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "enriched2.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+---------+------------------+\n",
      "|customer_id|recency|frequency|            amount|\n",
      "+-----------+-------+---------+------------------+\n",
      "|       6240|   3005|        3| 76.66666666666667|\n",
      "|      52800|   3320|        1|              15.0|\n",
      "|     100140|     13|        4|             51.25|\n",
      "|     109180|     30|        8|             48.75|\n",
      "|     131450|    205|        8|            103.75|\n",
      "|      45300|    234|        6|29.166666666666668|\n",
      "|      69460|     15|        9| 28.88888888888889|\n",
      "|      86180|      2|        9| 21.11111111111111|\n",
      "|     161110|   1528|        1|              30.0|\n",
      "|      60070|   2074|        3|51.666666666666664|\n",
      "|      13610|   1307|        8|           3043.75|\n",
      "|     100010|    413|        7|27.857142857142858|\n",
      "|     107930|    150|        5|              79.0|\n",
      "|     132610|     30|        7|28.571428571428573|\n",
      "|     154770|    427|        1|              45.0|\n",
      "|      49290|    371|        5|              24.0|\n",
      "|     229650|    419|        1|              45.0|\n",
      "|     220290|    366|        2|              40.0|\n",
      "|     178550|      3|        8|             56.25|\n",
      "|     112480|   1890|        3|38.333333333333336|\n",
      "+-----------+-------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "enriched2.createOrReplaceTempView(\"customers\")\n",
    "spark.sql(\"select * from customers\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "We need to address the following before we can train the model:\n",
    "\n",
    "1. Data Dispersion: The *amount* is skewed. We need to take **log** to address this skewness.\n",
    "2. Scale: Different features use different scales (see table). These features need to be standardized so that they contribute equally to the result.\n",
    "\n",
    "\n",
    "|Feature|Scale|\n",
    "|-------|-----|\n",
    "|recency|days|\n",
    "|frequency|purchase occassions|\n",
    "|amount|dollars|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Handle data dispersion**\n",
    "\n",
    "Below histogram shows that majority of the purchases are below 5 and 300 dollars. The data is left-skewed ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "startValues = Array(5.0, 304.6666666666667, 604.3333333333334, 904.0, 1203.6666666666667, 1503.3333333333333, 1803.0, 2102.6666666666665, 2402.3333333333335, 2702.0, 3001.6666666666665, 3301.3333333333335, 3601.0, 3900.6666666666665, 4200.333333333333, 4500.0)\n",
       "counts = Array(18126, 144, 28, 56, 16, 8, 19, 4, 3, 5, 1, 0, 0, 5, 2)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(18126, 144, 28, 56, 16, 8, 19, 4, 3, 5, 1, 0, 0, 5, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val (startValues, counts) = enriched2.select($\"amount\").map(v=>v.getDouble(0)).rdd.histogram(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us derive a new column, *log_amount*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+---------+-----------------+------------------+\n",
      "|customer_id|recency|frequency|           amount|        log_amount|\n",
      "+-----------+-------+---------+-----------------+------------------+\n",
      "|       6240|   3005|        3|76.66666666666667| 4.339467020255086|\n",
      "|      52800|   3320|        1|             15.0|  2.70805020110221|\n",
      "|     100140|     13|        4|            51.25|3.9367156180185177|\n",
      "|     109180|     30|        8|            48.75| 3.886705197443856|\n",
      "|     131450|    205|        8|           103.75| 4.641984159110808|\n",
      "+-----------+-------+---------+-----------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "enriched3 = [customer_id: string, recency: int ... 3 more fields]\n",
       "startValues = Array(1.6094379124341003, 2.062930896655721, 2.516423880877342, 2.9699168650989627, 3.423409849320583, 3.8769028335422036, 4.330395817763825, 4.783888801985445, 5.237381786207066, 5.690874770428687, 6.1443677546503075, 6.597860738871929, 7.051353723093549, 7.50484670731517, 7.958339691536791, 8.411832675758411)\n",
       "counts = Array(109, 987, 1567, 6989, 3004, 3291, 1453, 487, 217, 95, 81, 73, 26, 26, 12)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(109, 987, 1567, 6989, 3004, 3291, 1453, 487, 217, 95, 81, 73, 26, 26, 12)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val enriched3 = enriched2.withColumn(\"log_amount\", log($\"amount\"))\n",
    "val (startValues, counts) = enriched3.select($\"log_amount\").map(v=>v.getDouble(0)).rdd.histogram(15)\n",
    "enriched3.createOrReplaceTempView(\"customers\")\n",
    "spark.sql(\"select * from customers\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Scale**\n",
    "\n",
    "Distance algorithms like K-means (clustering) are most affected by the range of features because they are using distance between data points to determine their similarity.  So there is a chance that higher weights will be given to features with higher magnitude.\n",
    "\n",
    "**Standardization** is a technique used to scale our features so that all the features **contribute equally to the result**. Logic: (data - mean)/std-dev. This puts most of the data between -2 & 2.\n",
    "\n",
    "We will use [StandardScaler](https://spark.apache.org/docs/1.4.1/ml-features.html#standardscaler) from Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+---------+-----------------+------------------+--------------------+\n",
      "|customer_id|recency|frequency|           amount|        log_amount|            features|\n",
      "+-----------+-------+---------+-----------------+------------------+--------------------+\n",
      "|       6240|   3005|        3|76.66666666666667| 4.339467020255086|[3005.0,3.0,4.339...|\n",
      "|      52800|   3320|        1|             15.0|  2.70805020110221|[3320.0,1.0,2.708...|\n",
      "|     100140|     13|        4|            51.25|3.9367156180185177|[13.0,4.0,3.93671...|\n",
      "+-----------+-------+---------+-----------------+------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----------+-------+---------+-----------------+------------------+------------------------------+-----------------------------------------------------------+\n",
      "|customer_id|recency|frequency|amount           |log_amount        |features                      |std_features                                               |\n",
      "+-----------+-------+---------+-----------------+------------------+------------------------------+-----------------------------------------------------------+\n",
      "|6240       |3005   |3        |76.66666666666667|4.339467020255086 |[3005.0,3.0,4.339467020255086]|[1.6200302869727041,0.07410054463274243,0.9865583530258751]|\n",
      "|52800      |3320   |1        |15.0             |2.70805020110221  |[3320.0,1.0,2.70805020110221] |[1.911309156720168,-0.6068923348588853,-1.1388099972602492]|\n",
      "|100140     |13     |4        |51.25            |3.9367156180185177|[13.0,4.0,3.9367156180185177] |[-1.146656628215909,0.41459698437855624,0.4618640668707042]|\n",
      "+-----------+-------+---------+-----------------+------------------+------------------------------+-----------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "assembler = vecAssembler_e1c75f813c76\n",
       "features = [customer_id: string, recency: int ... 4 more fields]\n",
       "scaler = stdScal_2bbbe005b11a\n",
       "scalerModel = stdScal_2bbbe005b11a\n",
       "scaledFeatures = [customer_id: string, recency: int ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[customer_id: string, recency: int ... 5 more fields]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.StandardScaler\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "\n",
    "val assembler = new VectorAssembler()\n",
    "                    .setInputCols(Array(\"recency\", \"frequency\", \"log_amount\"))\n",
    "                    .setOutputCol(\"features\")\n",
    "\n",
    "val features = assembler.transform(enriched3)\n",
    "features.show(3)\n",
    "\n",
    "\n",
    "val scaler = new StandardScaler()\n",
    "                .setInputCol(\"features\")\n",
    "                .setOutputCol(\"std_features\")\n",
    "                .setWithStd(true)\n",
    "                .setWithMean(true)\n",
    "// Compute summary statistics by fitting the StandardScaler\n",
    "val scalerModel = scaler.fit(features)\n",
    "// Standardize each feature to have unit standard deviation.\n",
    "val scaledFeatures = scalerModel.transform(features)\n",
    "scaledFeatures.show(3, truncate=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
