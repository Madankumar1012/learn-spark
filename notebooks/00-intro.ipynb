{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective: Get used to the jupyter and Spark environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Have ScalaDoc handy. Open this url https://spark.apache.org/docs/latest/api/scala/index.html#package in a new tab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which Spark version are we using?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4.5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The Spark Web UI can be accessed from http://localhost:4040. Keep this open in another tab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us check important spark objects:\n",
    "\n",
    "1. `SparkSession`: entry point. You can have multiple sessions created for a single Spark Application.\n",
    "2. `SparkContext`: entry point before Spark 2.0\n",
    "3. `spark.Catalog`: interface to the Spark's *current* metastore. i.e. data catalog of relational entities like databases, tables, views, table columns & user-defined functions (UDF) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@9ea10b2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.SparkContext@331abc9d"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.SparkContext@331abc9d"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.internal.CatalogImpl@3346091b"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SparkSession` can be used to \n",
    "\n",
    "1. Create `DataFrame` or `DataSet`\n",
    "2. Interface with internal metastore, a.k.a data catalog\n",
    "3. Execute sql queries\n",
    "4. Access `DataFrameReader` to load datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Create DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id| fruit|\n",
      "+---+------+\n",
      "|  1| apple|\n",
      "|  2|banana|\n",
      "|  3|cherry|\n",
      "+---+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data = List([1,apple], [2,banana], [3,cherry])\n",
       "rdd = ParallelCollectionRDD[0] at parallelize at <console>:40\n",
       "schema = StructType(StructField(id,IntegerType,true), StructField(fruit,StringType,true))\n",
       "df = [id: int, fruit: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: int, fruit: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "//Define data as collection\n",
    "val data=Seq(\n",
    "    Row(1, \"apple\"),\n",
    "    Row(2, \"banana\"),\n",
    "    Row(3, \"cherry\")\n",
    "    )\n",
    "\n",
    "//Create a distributed collection i.e. RDD\n",
    "val rdd = sc.parallelize(data)\n",
    "\n",
    "//Define schema\n",
    "val schema = StructType(\n",
    "    Seq(\n",
    "          StructField(\"id\", IntegerType, true),\n",
    "          StructField(\"fruit\", StringType, true)\n",
    "    )\n",
    ")\n",
    "\n",
    "//Create DF\n",
    "val df = spark.createDataFrame(rdd, schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Interface with internal metastore, a.k.a data catalog**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us list databases and tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+-------------------------------------------+\n",
      "|name   |description     |locationUri                                |\n",
      "+-------+----------------+-------------------------------------------+\n",
      "|default|default database|file:/home/jovyan/notebooks/spark-warehouse|\n",
      "+-------+----------------+-------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.catalog.listDatabases().show(10, truncate=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----------+---------+-----------+\n",
      "|name|database|description|tableType|isTemporary|\n",
      "+----+--------+-----------+---------+-----------+\n",
      "+----+--------+-----------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.catalog.listTables().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if a database or table exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----------+------------------------------------------------------------+-----------+\n",
      "|name      |database|description|className                                                   |isTemporary|\n",
      "+----------+--------+-----------+------------------------------------------------------------+-----------+\n",
      "|!         |null    |null       |org.apache.spark.sql.catalyst.expressions.Not               |true       |\n",
      "|%         |null    |null       |org.apache.spark.sql.catalyst.expressions.Remainder         |true       |\n",
      "|&         |null    |null       |org.apache.spark.sql.catalyst.expressions.BitwiseAnd        |true       |\n",
      "|*         |null    |null       |org.apache.spark.sql.catalyst.expressions.Multiply          |true       |\n",
      "|+         |null    |null       |org.apache.spark.sql.catalyst.expressions.Add               |true       |\n",
      "|-         |null    |null       |org.apache.spark.sql.catalyst.expressions.Subtract          |true       |\n",
      "|/         |null    |null       |org.apache.spark.sql.catalyst.expressions.Divide            |true       |\n",
      "|<         |null    |null       |org.apache.spark.sql.catalyst.expressions.LessThan          |true       |\n",
      "|<=        |null    |null       |org.apache.spark.sql.catalyst.expressions.LessThanOrEqual   |true       |\n",
      "|<=>       |null    |null       |org.apache.spark.sql.catalyst.expressions.EqualNullSafe     |true       |\n",
      "|=         |null    |null       |org.apache.spark.sql.catalyst.expressions.EqualTo           |true       |\n",
      "|==        |null    |null       |org.apache.spark.sql.catalyst.expressions.EqualTo           |true       |\n",
      "|>         |null    |null       |org.apache.spark.sql.catalyst.expressions.GreaterThan       |true       |\n",
      "|>=        |null    |null       |org.apache.spark.sql.catalyst.expressions.GreaterThanOrEqual|true       |\n",
      "|^         |null    |null       |org.apache.spark.sql.catalyst.expressions.BitwiseXor        |true       |\n",
      "|abs       |null    |null       |org.apache.spark.sql.catalyst.expressions.Abs               |true       |\n",
      "|acos      |null    |null       |org.apache.spark.sql.catalyst.expressions.Acos              |true       |\n",
      "|add_months|null    |null       |org.apache.spark.sql.catalyst.expressions.AddMonths         |true       |\n",
      "|aggregate |null    |null       |org.apache.spark.sql.catalyst.expressions.ArrayAggregate    |true       |\n",
      "|and       |null    |null       |org.apache.spark.sql.catalyst.expressions.And               |true       |\n",
      "+----------+--------+-----------+------------------------------------------------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dbExists = true\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dbExists = spark.catalog.databaseExists(\"default\")\n",
    "val tblExists = spark.catalog.tableExists(\"test_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List all the functions in the \"default\" database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----------+------------------------------------------------------------+-----------+\n",
      "|name      |database|description|className                                                   |isTemporary|\n",
      "+----------+--------+-----------+------------------------------------------------------------+-----------+\n",
      "|!         |null    |null       |org.apache.spark.sql.catalyst.expressions.Not               |true       |\n",
      "|%         |null    |null       |org.apache.spark.sql.catalyst.expressions.Remainder         |true       |\n",
      "|&         |null    |null       |org.apache.spark.sql.catalyst.expressions.BitwiseAnd        |true       |\n",
      "|*         |null    |null       |org.apache.spark.sql.catalyst.expressions.Multiply          |true       |\n",
      "|+         |null    |null       |org.apache.spark.sql.catalyst.expressions.Add               |true       |\n",
      "|-         |null    |null       |org.apache.spark.sql.catalyst.expressions.Subtract          |true       |\n",
      "|/         |null    |null       |org.apache.spark.sql.catalyst.expressions.Divide            |true       |\n",
      "|<         |null    |null       |org.apache.spark.sql.catalyst.expressions.LessThan          |true       |\n",
      "|<=        |null    |null       |org.apache.spark.sql.catalyst.expressions.LessThanOrEqual   |true       |\n",
      "|<=>       |null    |null       |org.apache.spark.sql.catalyst.expressions.EqualNullSafe     |true       |\n",
      "|=         |null    |null       |org.apache.spark.sql.catalyst.expressions.EqualTo           |true       |\n",
      "|==        |null    |null       |org.apache.spark.sql.catalyst.expressions.EqualTo           |true       |\n",
      "|>         |null    |null       |org.apache.spark.sql.catalyst.expressions.GreaterThan       |true       |\n",
      "|>=        |null    |null       |org.apache.spark.sql.catalyst.expressions.GreaterThanOrEqual|true       |\n",
      "|^         |null    |null       |org.apache.spark.sql.catalyst.expressions.BitwiseXor        |true       |\n",
      "|abs       |null    |null       |org.apache.spark.sql.catalyst.expressions.Abs               |true       |\n",
      "|acos      |null    |null       |org.apache.spark.sql.catalyst.expressions.Acos              |true       |\n",
      "|add_months|null    |null       |org.apache.spark.sql.catalyst.expressions.AddMonths         |true       |\n",
      "|aggregate |null    |null       |org.apache.spark.sql.catalyst.expressions.ArrayAggregate    |true       |\n",
      "|and       |null    |null       |org.apache.spark.sql.catalyst.expressions.And               |true       |\n",
      "+----------+--------+-----------+------------------------------------------------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.catalog.listFunctions(\"default\").show(20, truncate=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Execute sql queries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Error parsing magics!\n",
       "Message: Magics [sql] do not exist!\n",
       "StackTrace: "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SHOW DATABASES\").show()\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Access DataFrameReader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-------------+-------------+----------+\n",
      "|id |name              |department_id|date_of_birth|company_id|\n",
      "+---+------------------+-------------+-------------+----------+\n",
      "|155|Dulce Rosse       |1            |1991-02-04   |26147     |\n",
      "|354|Imogene Marchand  |1            |1991-02-04   |146257    |\n",
      "|499|Cleveland Bonet   |1            |1993-12-04   |218370    |\n",
      "|505|Sharice Landrith  |1            |1991-02-04   |307860    |\n",
      "|511|Jacquelynn Gadbois|1            |1993-12-04   |24097     |\n",
      "+---+------------------+-------------+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "reader = org.apache.spark.sql.DataFrameReader@65bafc4a\n",
       "userDF = [id: string, name: string ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: string, name: string ... 3 more fields]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val reader = spark.read\n",
    "\n",
    "val userDF = reader.option(\"header\", \"true\").csv(\"../data/users.csv.gz\")\n",
    "userDF.show(5, truncate=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session runtime configuration\n",
    "\n",
    "We can modify (retrieve) `spark.sql.*` configuration parameters at runtime and per session using `spark.conf.set()`.\n",
    "\n",
    "NOTE: Depending on the deployment mode, some options (most notably spark.*.extraJavaOptions) cannot be set using this method, and can be modified only through `spark-submit` arguments or using configuration files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.driver.host = a9c6572bcfd9\n",
      "spark.driver.port = 45969\n",
      "spark.repl.class.uri = spark://a9c6572bcfd9:45969/classes\n",
      "spark.jars = file:/opt/conda/share/jupyter/kernels/apache_toree_scala/lib/toree-assembly-0.3.0-incubating.jar\n",
      "spark.repl.class.outputDir = /tmp/spark-6fcb36d9-9b79-4778-8d34-4886927c41d1/repl-61d11760-9d1b-45c9-a776-e3e3b1196d38\n",
      "spark.app.name = Apache Toree\n",
      "spark.executor.id = driver\n",
      "spark.driver.extraJavaOptions = -Dlog4j.logLevel=info\n",
      "spark.submit.deployMode = client\n",
      "spark.master = local[*]\n",
      "spark.app.id = local-1590885055845\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "conf = org.apache.spark.sql.RuntimeConfig@5174a669\n",
       "keyVal = Map(spark.driver.host -> a9c6572bcfd9, spark.driver.port -> 45969, spark.repl.class.uri -> spark://a9c6572bcfd9:45969/classes, spark.jars -> file:/opt/conda/share/jupyter/kernels/apache_toree_scala/lib/toree-assembly-0.3.0-incubating.jar, spark.repl.class.outputDir -> /tmp/spark-6fcb36d9-9b79-4778-8d34-4886927c41d1/repl-61d11760-9d1b-45c9-a776-e3e3b1196d38, spark.app.name -> Apache Toree, spark.executor.id -> driver, spark.driver.extraJavaOptions -> -Dlog4j.logLevel=info, spark.submit.deployMode -> client, spark.master -> local[*], spark.app.id -> local-1590885055845)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map(spark.driver.host -> a9c6572bcfd9, spark.driver.port -> 45969, spark.repl.class.uri -> spark://a9c6572bcfd9:45969/classes, spark.jars -> file:/opt/conda/share/jupyter/kernels/apache_toree_scala/lib/toree-assembly-0.3.0-incubating.jar, spark.repl.class.outputDir -> /tmp/spark-6fcb36d9-9b79-4778-8d34-4886927c41d1/repl-61d11760-9d1b-45c9-a776-e3e3b1196d38, spark.app.name -> Apache Toree, spark.executor.id -> driver, spark.driver.extraJavaOptions -> -Dlog4j.logLevel=info, spark.submit.deployMode -> client, spark.master -> local[*], spark.app.id -> local-1590885055845)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val conf = spark.conf\n",
    "\n",
    "val keyVal = conf.getAll\n",
    "\n",
    "keyVal.foreach(entry => println(entry._1 +\" = \"+entry._2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark and Metadata Catalogs\n",
    "\n",
    "One of the needs of structured data analysis is metadata management (location of the data, comments, statistics or schema etc). The metadata can be **temporary metadata** like temp table, registered udfs on SQL context or **permanent metadata**. Spark provides `Catalog` abstraction (since 2.0.0) to interact with the metadata stores like Hive meta store or HCatalog.\n",
    "\n",
    "Spark, by default, comes with the following three implementations of the `Catalog` abstraction\n",
    "\n",
    "1. InMemoryCatalog\n",
    "    - ephemeral, used for learning & testing\n",
    "2. HiveExternalCatalog\n",
    "    - Used when you `enableHiveSupport()` on `SparkSession`\n",
    "    - Internally uses `HiveClient` & `HiveClientImpl` to interact with Hive metastore\n",
    "    - Wrapper on top of Hadoop package `org.apache.hadoop.hive.ql.metadata`\n",
    "3. ExternalCatalogWithListener\n",
    "    - Since 2.4.0\n",
    "    - Just a wrapper around #1 or #2\n",
    "    - Added functionality of posting catalog related events (Table created, deleted etc.) to the Spark's *listener bus*\n",
    "    \n",
    "A Hive **metastore warehouse** (aka `spark-warehouse`) is the directory where Spark SQL persists **data** in tables whereas a Hive **metastore** (aka `metastore_db`) is a relational database to manage the **metadata** of the persistent relational entities, e.g. databases, tables, columns, partitions.\n",
    "\n",
    "Hive works \n",
    "By default, Spark SQL uses the embedded deployment mode of a Hive metastore with a Apache Derby database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:/home/jovyan/notebooks/spark-warehouse/\n",
      "in-memory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lastException = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Name: java.util.NoSuchElementException\n",
       "Message: \n",
       "StackTrace:   at org.apache.spark.sql.internal.SQLConf$$anonfun$getConfString$2.apply(SQLConf.scala:2042)\n",
       "  at org.apache.spark.sql.internal.SQLConf$$anonfun$getConfString$2.apply(SQLConf.scala:2042)\n",
       "  at scala.Option.getOrElse(Option.scala:121)\n",
       "  at org.apache.spark.sql.internal.SQLConf.getConfString(SQLConf.scala:2042)\n",
       "  at org.apache.spark.sql.RuntimeConfig.get(RuntimeConfig.scala:74)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(spark.conf.get(\"spark.sql.warehouse.dir\"))\n",
    "println(spark.conf.get(\"spark.sql.catalogImplementation\"))\n",
    "println(spark.conf.get(\"\"))\n",
    "//println(spark.conf.get(\"\"))\n",
    "//spark.sql.hive.metastore.jars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
